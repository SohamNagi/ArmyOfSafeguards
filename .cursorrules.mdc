---
alwaysApply: false
---
# .cursorrules
# ============================================================
# ðŸ§  Army of Safeguards Project (Phase 1)
# ============================================================
# Cursor configuration for the Army of Safeguards research project.
# The goal: build a modular multi-critic system that detects and mitigates
# undesirable behaviors in LLM outputs (e.g., misinformation, toxicity,
# sexual content, jailbreaks). Each critic runs independently and can be
# called from a central aggregator.

project:
  name: "Army of Safeguards"
  purpose: >
    A modular AI safety framework consisting of multiple lightweight
    language models (â€œcriticsâ€) that evaluate LLM outputs for safety and
    factuality before final delivery.
  context: >
    Developed for UR2PhD / University of Waterloo 
    research collaboration. Phase 1 focuses on building standalone safeguards
    and integrating them under a single aggregator pipeline.

structure:
  - factuality/            # Ajith - factuality / misinformation critic
  - toxicity/              # Soham - hate / toxicity critic
  - sexual/                # Jian - sexual content critic
  - jailbreak/             # Tommy - jailbreak intent critic
  - aggregator/            # Shared unified interface
  - tests/                 # Unit tests, benchmarks, and QA
  - requirements.txt       # Shared dependencies
  - README.md              # Root project documentation
  - STRUCTURE.md           # Architectural explanation

principles:
  - Modularity: Each safeguard is self-contained (has its own folder, README, and predict() function).
  - Standardized interface: All safeguards must expose:
      - predict(text: str) -> {"label": str, "confidence": float}
      - Optional aggregate(predictions: list) -> {"label": str, "confidence": float}
  - Reusability: The aggregator imports all safeguards dynamically.
  - Transparency: Every model must have a linked Hugging Face card describing dataset, training, and metrics.
  - Safety first: No generation or modification of unsafe content.

rules:
  - Do not mix logic between safeguards â€” each module only handles its own task.
  - Do not hardcode file paths; use relative imports with `Path(__file__).parent`.
  - Use lightweight transformer models (< 150M parameters).
  - Prefer Hugging Faceâ€™s `datasets` + `transformers` APIs.
  - Do not add any UI or web framework; keep the system CLI + Python only.
  - Add new dependencies only through `requirements.txt` at repo root.
  - Tests go in `tests/` and should run independently.

coding_conventions:
  - Use snake_case for all function and variable names.
  - Include docstrings explaining each functionâ€™s purpose.
  - Keep safeguard scripts under 300 lines if possible.
  - Use `if __name__ == "__main__":` blocks for CLI demos.
  - Return structured JSON-like dictionaries for all outputs.
  - Prefer explicit imports (no wildcard imports).

naming:
  - safeguard_<task>.py for critic files.
  - test_<task>.py for corresponding unit tests.
  - aggregator.py for the unified system script.

